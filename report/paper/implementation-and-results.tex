\chapter{Implementation, Analysis and Results}\label{implementation_analysis}
In the following two sections, we implement the methods and share the analysis results for the steel production and simulation events. Creating sets of experimental data from the simulation allows comparing the statistical characteristics of their association network with those formed from the production data sets from steel manufacturing.
\section{Steel Production Events}
\subsection{Data Collection and Cleaning}\label{data_collection_cleaning}
%\addcontentsline{toc}{subsection}{Data Collection and Cleaning}%

We started to cleaning process by converting string-type data into floating-point numbers, modifying inconsistent punctuation marks between digits into one typical style and converting null values into the integer value $0$. After going over minor revision steps, we introduce some preconditions below to disregard data errors and fill the gaps in the data sets.
\begin{itemize}
	\item The steel material density was calculated for every event, and a range was taken into account between $6.5 x 10^{-6}$ $kg/mm^{3}$ and $8.5 x 10^{-6}$ $kg/mm^{3}$. The production orders with density values out of that range were discarded from consideration.
	\item The input capacity of machines was identified for width and weight as in the range of $800$--$2000$ mm, and $2669$--$26690$ kg, which allowed to perform necessary unit conversions correctly.
	\item The unit of length values was considered as millimetre (mm). In case of need, length values were converted into suitable magnitudes based on the above given two preconditions.
\end{itemize}
Zeros were replaced with calculated values for every event with a maximum of one unknown value considering the preconditions mentioned above and the $density = mass/volume$ equation. The events with two zero values were compared with consecutive events, and missing terms were filled based on the consistency among the same attribute values. Sequences with less than $50$ events were removed from the data sets considering those short sequences might be generated for some test processes.

\input{../figures/figure-PLTCM_attributes}
\input{../figures/figure-CGL_attributes}

The product features differ before and after the handling by production lines. We are interested in having the feature values before the events are treated in machines to capture the effective patterns in sequence planning concerning our hypothetical constraints. In the PLTCM data collection, width, thickness and length features have additional attributes as labelled with $input$ and $target$ for each, and their histograms are given in Fig.~\ref{figure-data-cleaning-PLTCM}. We considered $input$ labelled attributes for PLTCM in our calculations. Unlike PLTCM, the CGL data set has single attributes for dimension features, which we assume as the output values. Based on the reason explained above, we matched the events in the CGL data set with the ones given in PLTCM data set by the attributes, $Material\_ID$ and $Piece\_ID$. The first column in Fig.~\ref{figure-data-cleaning-CGL} shows the resulting PLTCM data set thickness and width values that we used as $input$ values for CGL.

At the final stage, obtained data set lengths are given below.
\begin{itemize}
	\item \acs{ccm} data set: $347,418$ events.
	\item \acs{csp} data set: $205,496$ events.
	\item \acs{pltcm} data set: $59,604$ events.
	\item \acs{cgl} data set: $27,147$ events.
\end{itemize}

\subsection{Analysis Steps and Results}
%\addcontentsline{toc}{subsection}{Analysis Steps and Results}%

The complete analysis of steel production events comprises a check for meaningful modularity structures in various dimensions for the association networks generated from the data collection. We introduce those dimensions as;
\begin{itemize}
	\item[1.] production line,
	\item[2.] production feature,
	\item[3.] production constraint,
	\item[4.] null model,
	\item[5.] time resolution, and
	\item[6.] network resolution.
\end{itemize} 
For the first dimension, distinguished data sets belonging to \acs{ccm}, \acs{csp}, \acs{pltcm}, and \acs{cgl} were considered. The following dimensions were examined independently in each production line.

Since width and thickness are the products' physical quantities that are deliberately reformed during the whole manufacturing process, those data feature columns are taken into account for each production line to be analysed as the second dimension. 

As we argued previously, two fundamentally different constraints, technology-driven constraints and load-driven constraints, act on the manufacturing process. Alternative binning methods and different network approaches were identified for those constraints. \acs{fss} and \acs{fbs} networks were generated in the third dimension for each production feature in every production line.

\acs{nmd} and \acs{nmm}, as previously introduced, were considered to check the randomness of the association networks. As the fourth dimension, those alternative null models were constructed for each \acs{fss} and \acs{fbs} network generated. The resulting z-scores are more straightforward than the resulting modularity values since they take out any effect from different link densities. For this reason, we shared only z-scores as bar charts in this subsection and attached the bar charts and curve plots for the modularity values as supplementary materials.

Time resolution is the fifth dimension, and it consists of different observation-window categories as discrete-time windows, sliding-time windows and complete data with two halves. Each resolution is a means of partitioning the data of historically ordered production events into equal sizes differently. For each window in three categories, the first four dimensions were performed. The analysis with time-resolved fashion allows checking if any significant constraint impact occurs systematically through the time windows created with varying sizes. Since the most significant results were obtained from the last category, the complete data with two halves, we shared and discussed that category in this subsection. The analysis results for discrete-time windows and sliding-time windows are supplementary materials: \ref{figure-supplements-CCM_CSP-curveplots_discrete}, \ref{figure-supplements-PLTCM_CGL-curveplots_discrete} and \ref{figure-supplements-CCM-curveplots_sliding}, \ref{figure-supplements-CSP-curveplots_sliding}, \ref{figure-supplements-PLTCM-curveplots_sliding}, \ref{figure-supplements-CGL-curveplots_sliding}.

As the last dimension, association networks were obtained from the first four dimensions in the observation-window categories. The sliding-time windows and the complete data with two halves were diversified in two different network resolutions by changing the node number. We achieved this by choosing the appropriate step and bucket sizes while generating graphs. We aimed to obtain the maximum number of nodes to quantify the modularity and keep the node numbers the same in different network approaches in the respective time window. Other than the \acs{csp} Thickness and \acs{ccm} Thickness networks, including fewer nodes than $15$ in some cases, all networks have varying node numbers between $25$--$90$. The resulted plots for four production lines in alternative network resolutions are presented in supplementary materials: \ref{figure-supplements-CCM-curveplots_sliding}, \ref{figure-supplements-CSP-curveplots_sliding}, \ref{figure-supplements-PLTCM-curveplots_sliding}, \ref{figure-supplements-CGL-curveplots_sliding} and \ref{figure-supplements-CCM}, \ref{figure-supplements-CSP}, \ref{figure-supplements-PLTCM}, \ref{figure-supplements-CGL}.

We pretend that the real network structure is more homogeneous than it is and pretend that it is statistically reliable. However, those assumptions are slightly wrong, and those can sometimes lead to cases looking statistically significant even though they are not. An error bar was included in each z-score bar to distinguish the reliably distributed network from the non-robust ones. The error bar is the mean value of the respective z-score by removing and putting back 10\% of the data ten times to check the robustness of the statistical signals. The T-shaped symbol represents the standard deviation of the error bars.

Condensed analysis results as bar charts are given in Fig.~\ref{figure-real-life-events-analysis-results}, showing z-scores concerning alternative null models in different network approaches for width and thickness features of four production lines. In the bar charts, z-scores are indicated with a colourless line finish, and error bars were included in colour. Green lines are indicated on the values: $+1$ and $-1$ as the significance thresholds for signals as one standard deviation range.
\renewcommand{\aa}{Analysis Bar Chart Results:}
\input{../figures/figure-real_life-events-analysis-results}

\newcommand{\bb}{Modularity Values and Z-scores in Different Network Resolutions}
\newcommand{\cc}{Analysis Curve Plot Results:}
\newcommand{\dd}{Modularity Values and Z-scores in Sliding-time Windows \& Different Network Resolutions}
\newcommand{\ee}{Modularity Values and Z-scores in Discrete-time Windows}

At first glance on the bar charts, time does not influence modularity change except a few minor differences between the first halves and second halves of the data sets; the complete data column provides the overview of both halves. Moreover, \acs{ccm}--\acs{csp} and \acs{pltcm}--\acs{cgl} pairs have significantly similar results in terms of z-scores for different null models. That case is not unexpected for the first pair since \acs{ccm} is an inner \acs{csp} module; hence, the same production sequence schedule is performed on both production lines.

FSS networks for the width feature in CCM and CSP are always modular and hierarchically organised mainly. On the other hand, FBS networks of the same feature in the same production lines have nonrandom and non-modular hierarchical organisation after checking the robustness. Until perturbing the data, the networks are modular and hierarchical, showing the unstable case of the FBS networks for the width feature in CCM and CSP. FSS networks are modular and organised simple, while FBS networks are non-modular and hierarchically organised regarding the thickness feature in the same production lines. 

For the thickness feature in PLTCM and CGL, FSS and FBS networks are very modular. In CGL, they do not comprise any complex textures, and they are robust, whereas they have a hierarchical organisation in PLTCM. For width feature in PLTCM, FSS and FBS networks are again modular and have non-robust hierarchical organisation due to the error bars with long T-shaped symbols. For the width feature in CGL, FSS networks are not modular and hierarchical, while FBS networks have non-robust modular and hierarchical organisation.

Fig.~\ref{figure-real-life-events-analysis-results} shows us that alternative network approaches: FSS and FBS, concerning technology-driven constraints and load-driven constraints, give different results. The difference is rather significant for the width--thickness features in CCM--CSP and the width feature in CGL, whereas it is not very significant for the width--thickness features in PLTCM and the thickness feature in CGL.

In our opinion, FBS networks would have significantly differed from FSS networks for the length and weight features in PLTCM and CGL since those features have high diversity on the production events. We also argue that they would significantly impact load constraints in PLTCM and CGL compared to thickness and width features based on alternative handling categories introduced in Fig.~\ref{figure-hypothetical_constraints}.
\clearpage

\section{Simulation Events}
\subsection{Design Steps for the Simulation Model}
%\addcontentsline{toc}{subsection}{Design Steps for the Simulation Model}%

As summarised in Fig.~\ref{figure-complete_framework_cartoon}, the simulation model performs a single run optimisation with a defined set of rules and system constraints in the first step. As a second step, it generates the data set based on the introduced production sequence concept. Finally, the outcome data set is labelled in alternative binning schemes to construct networks as ready for the network metrics analysis. In this subsection, we quantify the characteristics of the model and vary the identified system constraints used in the second step.

Homo sapiens metabolic model with $738$ metabolites and $1008$ reactions was used as the set of rules (Eq.~\ref{stoichio}) and kept fixed in our simulation model. The linear optimisation algorithm was run $10000$ times to create a data set with $200$ sequences, each with $50$ events.

A constrained flux list, $V^{b}$ (Eq.~\ref{constrainedfluxlist}), was created to be considered in each optimisation run. The fluxes used in the intermediate reactions were given the range of bounds as $(-500, 500)$ since it is impossible to define infinity values in the optimisation algorithm. The bound variable, $a$, was assigned to $5$ for randomly chosen $105$ fluxes out of $1008$ as the fluxes used in uptake and secretion reactions.

Regarding the previously introduced production sequence concept, biomass (Eq.~\ref{biomassmaximisation}) was designed slightly sparse, consists of randomly selected fluxes matched with non-zero coefficients dedicated to each sequence instead of all $1008$ fluxes. The counts of biomass series for $200$ sequences is attached as supplementary material:~\ref{figure-supplements-flux_vector_series_counts}.

We recall the \acf{ppd} and \acf{ru} introduced in Section~\ref{section: simulation_model} to vary the system constraints and derive design steps as the basis of the simulation events analysis;
\begin{itemize}
	\item[i.] \acs{ppd}-1 picks random floating-point numbers from four different intervals; $(-1, 1)$, $(-4, 4)$, $(-4, -2)$, and $(2, 4)$ as the objective function coefficients (Eq.~\ref{objectivecoefficients}) which results as four distinctive data sets after the optimisation runs completed,
	\item[ii.] \acs{ru}-1 generates distinguished deletion lists ($V^{e}$, Eq.~\ref{deletedreactions}), each consists of randomly picked fluxes with the numbers range from $50$ to $450$ in step $50$ and considers those in the four distinctive data sets mentioned above to generate nine different variations of each,
	\item[iii.] \acs{ru}-2 updates $105$ to $420$, which was previously defined as the number of randomly chosen fluxes in $V^{b}$ to be limited with bound value, $a$, results with one more variation of the above steps, and
	\item[iv.] \acs{ppd}-2 creates three revised versions of the designed biomass series by randomly choosing the reduced number of terms by 25\%, 50\%, and 75\%, respectively, resulting in three more variations of the above design steps.
\end{itemize}

In short, \acs{ru}-1 and \acs{ru}-2 manipulate the simulation model's resource usage by knocking some reactions out and limiting fluxes used for uptake \& secretion reactions. \acs{ppd}-1 enforces the production plans based on their directionality by imposing symmetric and nonsymmetric number intervals. \acs{ppd}-2 provides varieties between a vast production range capacity and a more specific one by adjusting the richness of objective functions. %By deleting a high percentage of the terms in objective functions, we increase the constraint that the production plan imposes on the whole production process. 

\subsection{Analysis Results}
%\addcontentsline{toc}{subsection}{Analysis Results}%
Derived data sets from the design steps introduced in the above subsection were considered to construct networks concerning alternative binning methods, calculate modularity value of the networks and calculate z-scores concerning two alternative null models as performed in steel production events analysis.

The resulted curve plots are presented as supplementary materials: \ref{figure-supplements-obj_func-terms-initial}, \ref{figure-supplements-obj_func-terms-reduced25}, \ref{figure-supplements-obj_func-terms-reduced50}, and \ref{figure-supplements-obj_func-terms-reduced75}. As the deleted reaction number increases, modularity shifts down together with a significant decrease in \acs{nmd} z-scores. In contrast, the \acs{nmm} z-scores are always around zero. A condensed version of those resulted curve plots was re-arranged by taking the mean of both modularity values and z-scores in each data set variation derived from the RU-2 and PPD-2 steps. The results are given in Fig.~\ref{figure-FBA-results}.

None of the curves in \acs{fss} networks has a change in modularity. In contrast, \acs{fbs} networks modularity increases for only green and orange coloured curves. From left to right, we increase the constraint that the production plan (or product portfolio) impose on the whole production process. The product portfolio is grouped into speciality products on the right end. The effect of the portfolio changing occurs for the production plans enforced in one direction and does not occur for the symmetrically organised production plans. The red and blue curves are less attractive and instead serve as an orientation since their objective function coefficients are symmetric around zero and damp each other, resulting in a not occurring product. Therefore, we do not expect them to be severely affected by changes in the richness of the objective function, and that is indeed what we numerically observed.

\input{../figures/figure-FBA-results}

