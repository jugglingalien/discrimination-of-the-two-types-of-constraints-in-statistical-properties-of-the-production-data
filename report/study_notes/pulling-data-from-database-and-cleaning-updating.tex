\documentclass{article}
\usepackage{listings}
\begin{document}

Upon to two distinctive constraint definitions in my advance project 2 report, checking those hypothetical terms in real life data is decided. To be able to observe interesting patterns, a big data set with 2-3 years production orders is agreed to be investigated through time windows. 

I and Daniel started to discuss about the relevant features to be considered in this data set. At final stage below given SQL query was generated to pull the data set from the SMS database. The resultant data set consists of 459203 rows and 15 columns. First two columns and $4^{th}$ column features: ROS.R\textunderscore OS\textunderscore ID, ROS.PRODUCTION\textunderscore LINE\textunderscore NAME, and ROS.REFERENCE\textunderscore DATE come from "Reporting data: Operation step" table. $3^{rd}$ column feature SEQUENCE\textunderscore ID is actual casting sequence ID from the table "Reporting data: additional data of CCM (explain this)". $5^{th}$., $6^{th}$., $7^{th}$. and $14^{th}$. SLAB.PIECE\textunderscore ID, SLAB.MATERIAL\textunderscore ID, SLAB.MOLD\textunderscore WIDTH, and SLAB.EXIT\textunderscore TEMP come from "Reporting data: additional data of CCM which are slab related". Rest of the columns: MAT.WIDTH, MAT.THICKNESS, MAT.WEIGHT, MAT.LENGTH, MAT.HEAR\textunderscore ID, MAT.STEEL\textunderscore GRADE\textunderscore ID\textunderscore INT, and MAT.SLAB\textunderscore TRANSITION come from "Material ;  For slabs, coils, plates and heats" table.

\begin{lstlisting}
SELECT  ros.r_os_id, ros.production_line_name, ccm.sequence_id,        
        ros.reference_date, NVL( TO_CHAR(slab.piece_id),'NA') 
        piece_id, NVL( TO_CHAR(slab.material_id),'NA') material_id, 
        NVL(TO_CHAR(slab.mold_width),'NA') mold_width, 
        NVL( TO_CHAR(mat.width),'NA') width, 
        NVL( TO_CHAR(mat.thickness),'NA') thickness, 
        NVL( TO_CHAR(mat.weight),'NA') weight, 
        NVL( TO_CHAR(mat.length),'NA')
        length, NVL( TO_CHAR(mat.heat_id),'NA') heat_id, 
        NVL( TO_CHAR(mat.steel_grade_id_int),'NA') steel_grade_id_int, 
        NVL( TO_CHAR(slab.exit_temp),'NA') exit_temp, 
        NVL( TO_CHAR(mat.slab_transition),'NA') slab_transition

FROM   	L3MAIN.r_os ros 
        LEFT JOIN L3MAIN.r_ccm ccm ON ros.r_os_id = ccm.r_os_id 
        LEFT JOIN L3MAIN.r_ccm_slab slab ON ros.r_os_id = slab.r_os_id 
        LEFT JOIN L3MAIN.r_mat mat ON ros.r_os_id = mat.r_os_id 

WHERE  	sequence_id IS NOT NULL;
\end{lstlisting}

Parsed data belongs to CCM (Compact Cold Mill) production line.

$7.85 g/cm^{3}$ = $7850 kg/m^{3}$ = $0.284 lb/in^{3}$ = $490 lb/ft^{3}$

Converting strings to numbers and correction for punctuation marks between digits were performed, null values (NA) were converted into $0$ values in the beginnning of data cleaning process. After completing minor stages, some preconditions were generated as below to be able to manipulate data columns.
Steel density is considered between $7.00 x 10^{-6}$ $kg/mm^{3}$ and $8.50x10^{-6}$ $kg/mm^{3}$.
Width varies between 800 - 2000 mm. 
Thickness varies between 40 - 90 mm. 
Weight varies between 2669 - 26690 kg.
Length unit is mm.

Starting to modify width, thickness, and weight values corresponding to thickness values with 2 digits.

The data set has below given shape just before starting to analysis.
Weight Zero Rows: 10484
Thickness + Width + Weight Zero Rows: 61320
The rows with densities that do not match within above mentioned interval: 1787
Usable Rows: 396096

Time Windows Generation by Data Partitioning:

the dataset with length 396096 was partitioned in 10 time windows starting from the beginning of the data. In each step, it's increased by 39610 rows more or less (increasing windows). The exact increase step dimension was specified by the last order of corresponding sequence. For my dataset, exact time window lengths are 39871, 79567, 118358, 158421, 198041, 237352, 277147, 316411, 356385, 396096. Almost always same statistics for every window. Strange increase modularity increase towards the end due to increasing window size. If there is a shift in the way the data behave, I will almost not see it because it is mast by the other data that still be present in my analysis. The modularity curves seem drift upwards little bit. There is a trend of going up now matter how it behaves in the middle. My reason was to do this to check the load effect.

Partitioning was repeated with discrete time windows (sliding windows). Shifting window within equal windows size. To see the results of same analysis in each discrete time window. Whether the rules I discovered from the first dataset (1st time window) and the second dataset (2nd time window) are really fundamentally different or rather the same.






\end{document}